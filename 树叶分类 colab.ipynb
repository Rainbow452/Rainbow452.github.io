{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6977c7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install git+https://github.com/d2l-ai/d2l-zh@release  # installing d2l\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76afc0ba",
   "metadata": {},
   "source": [
    "## 安装kaggle、凭证（$\\color{#FF0000}{记得转入kaggle.json}$）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd57330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "命令语法不正确。\n",
      "'cp' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n",
      "'chmod' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73053 sha256=4d81e0c34427d24c19f641c82c28a8bcda858cdf3855e1827639648b9ad5a3c4\n",
      "  Stored in directory: c:\\users\\arita.sun\\appdata\\local\\pip\\cache\\wheels\\29\\da\\11\\144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.5.12\n",
      "    Uninstalling kaggle-1.5.12:\n",
      "      Successfully uninstalled kaggle-1.5.12\n",
      "Successfully installed kaggle-1.5.12\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/kaggle.json  /root/.kaggle\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "\n",
    "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
    "!kaggle competitions download -c classify-leaves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f0f9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26197aaf",
   "metadata": {},
   "source": [
    "##  解压数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac03709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def un_zip(file_name):\n",
    "    \"\"\"unzip zip file\"\"\"\n",
    "    base_dir = os.path.dirname(file_name)\n",
    "    zip_file = zipfile.ZipFile(file_name, 'r')\n",
    "    os.makedirs(os.path.join(*file_name.split('.')[:-1])   ,exist_ok=True )\n",
    "    zip_file.extractall(os.path.join(*file_name.split('.')[:-1]) )\n",
    "\n",
    "un_zip('classify-leaves.zip')\n",
    "len(os.listdir('classify-leaves'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ebce4e",
   "metadata": {},
   "source": [
    "## 数据集处理01(训练集csv 测试集csv-->将数据放到指定文件夹)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a08d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集 处理\n",
    "def make_dataset(fpath, tpath = 'data'):\n",
    "    \n",
    "    \n",
    "    data = pd.read_csv(fpath) # csv\n",
    "    os.makedirs(os.path.join(tpath), exist_ok=True)\n",
    "    for i in range(data.shape[0]):\n",
    "        if data.shape[1] == 2: #训练集\n",
    "            fname = os.path.join(tpath, 'trainset')# 训练集目标位置\n",
    "            os.makedirs(fname, exist_ok=True)\n",
    "            os.makedirs(os.path.join(fname, data.iloc[i,1]), exist_ok=True)\n",
    "            shutil.copy(os.path.join(*fpath.split('/')[:-1], data.iloc[i, 0]), os.path.join(fname, data.iloc[i,1]))\n",
    "            \n",
    "        elif data.shape[1] == 1: #测试集\n",
    "            fname = os.path.join(tpath, 'testset')\n",
    "            os.makedirs(fname, exist_ok=True)\n",
    "            print(os.path.join( *fpath.split('/')[:-1], data.iloc[i, 0]))\n",
    "            shutil.copy(os.path.join( *fpath.split('/')[:-1], data.iloc[i, 0]), fname)\n",
    "        else:\n",
    "            assert 'make dataset error!'\n",
    "\n",
    "make_dataset(os.path.join('classify-leaves', 'train.csv'), 'data')\n",
    "make_dataset(os.path.join('classify-leaves', 'test.csv'), 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce18f4",
   "metadata": {},
   "source": [
    "##  数据集处理02(数据放到指定文件夹 -->按比例随机将数据放入文件夹)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8935a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_testset(fpath, tpath, scale=.0): \n",
    "    '''func:将完整训练数据随机按scale 比例分成 train 和 test 文件夹\n",
    "        parmarater:\n",
    "        fpath:完好的训练数据文件夹， \n",
    "        tpath:目标文件夹'''\n",
    "    \n",
    "    os.makedirs(os.path.join(tpath), exist_ok=True)\n",
    "    os.makedirs(os.path.join(tpath, 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(tpath, 'test'), exist_ok=True)\n",
    "    class_name = os.listdir(fpath)\n",
    "#     num_scale = len(list_name)  #总共类别\n",
    "    for class_leaves in class_name:\n",
    "        imgname = os.listdir(os.path.join(fpath, class_leaves))  #每一类数据的文件夹\n",
    "        testdir = os.path.join(tpath, 'test', class_leaves)\n",
    "        traindir = os.path.join(tpath, 'train', class_leaves)\n",
    "        \n",
    "        os.makedirs(testdir, exist_ok=True)\n",
    "        os.makedirs(traindir, exist_ok=True)\n",
    "        for i in range(int(len(imgname) * scale)):\n",
    "            j = random.randint(0, len(imgname) -1)\n",
    "            \n",
    "            shutil.copy(os.path.join( fpath, class_leaves, imgname[j]), testdir)\n",
    "            del imgname[j]\n",
    "        for img in imgname:\n",
    "            shutil.copy(os.path.join( fpath, class_leaves, img), traindir)\n",
    "        \n",
    "        \n",
    "make_testset(os.path.join('data', 'trainset'),'train', scale=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f731a",
   "metadata": {},
   "source": [
    "## resnet-11 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9e0c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels, \n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, \n",
    "                               kernel_size=3, padding=1)\n",
    "        \n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels, \n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86a555a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(\n",
    "    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,),\n",
    "    nn.BatchNorm2d(64),nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "425e358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(input_channels, num_channels, num_residuals, \n",
    "                first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels, \n",
    "                               use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7b8e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "b5 = nn.Sequential(*resnet_block(256, 512, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81b475b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net =nn.Sequential(b1, b2, b3, b4, b5, \n",
    "                  nn.AdaptiveAvgPool2d((1,1)),\n",
    "                  nn.Flatten(), nn.Linear(512, 176))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "303d6b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 128, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 256, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c636e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch6(net, trian_iter, test_iter, num_epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('trianing on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1,num_epochs],\n",
    "                           legend=['train loss', 'trian acc', 'test acc'])\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i+1) / num_batches, (train_l, train_acc, None))\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))                        \n",
    "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}',\n",
    "         f'test_acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] *num_epochs / timer.sum():.1f} examples/sec',\n",
    "         f'on {str(device)}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa385899",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5acab9d0f79e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#图片处理流水线 resize 224, 转为向量 归一化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'trainset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'testset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "#图片处理流水线 resize 224, 转为向量 归一化\n",
    "transform = transforms.Compose([transforms.Resize(224), transforms.ToTensor()])\n",
    "train_dataset = datasets.ImageFolder(os.path.join('train', 'train'), transform=transform)\n",
    "test_dataset = datasets.ImageFolder(os.path.join('train', 'test'), transform=transform)\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset,batch_size=32, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset,batch_size=32, shuffle=False)\n",
    "\n",
    "images = next(iter(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9c35bac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-9a4ba4ccb8ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.05, 10, 32\n",
    "\n",
    "\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "原始单元格格式",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
